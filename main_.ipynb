{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psycopg2-binary==2.9.7 in c:\\users\\assane sarr\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from -r requirements.txt (line 1)) (2.9.7)\n",
      "Requirement already satisfied: googlemaps==4.10.0 in c:\\users\\assane sarr\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from -r requirements.txt (line 2)) (4.10.0)\n",
      "Requirement already satisfied: sklearn3==0.0.1 in c:\\users\\assane sarr\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from -r requirements.txt (line 3)) (0.0.1)\n",
      "Requirement already satisfied: boto3==1.28.28 in c:\\users\\assane sarr\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from -r requirements.txt (line 4)) (1.28.28)\n",
      "Requirement already satisfied: requests<3.0,>=2.20.0 in c:\\users\\assane sarr\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from googlemaps==4.10.0->-r requirements.txt (line 2)) (2.31.0)\n",
      "Requirement already satisfied: botocore<1.32.0,>=1.31.28 in c:\\users\\assane sarr\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from boto3==1.28.28->-r requirements.txt (line 4)) (1.31.28)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\assane sarr\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from boto3==1.28.28->-r requirements.txt (line 4)) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in c:\\users\\assane sarr\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from boto3==1.28.28->-r requirements.txt (line 4)) (0.6.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\assane sarr\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from botocore<1.32.0,>=1.31.28->boto3==1.28.28->-r requirements.txt (line 4)) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in c:\\users\\assane sarr\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from botocore<1.32.0,>=1.31.28->boto3==1.28.28->-r requirements.txt (line 4)) (1.26.16)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\assane sarr\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from requests<3.0,>=2.20.0->googlemaps==4.10.0->-r requirements.txt (line 2)) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\assane sarr\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from requests<3.0,>=2.20.0->googlemaps==4.10.0->-r requirements.txt (line 2)) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\assane sarr\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from requests<3.0,>=2.20.0->googlemaps==4.10.0->-r requirements.txt (line 2)) (2023.7.22)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\assane sarr\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.32.0,>=1.31.28->boto3==1.28.28->-r requirements.txt (line 4)) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "for i in range(0,len(df.center_lat)):\n",
    "    #match = re.search(pattern, polygon_str)\n",
    "    coordinates_list = [(df.center_lat[i], df.center_lon[i])]\n",
    "    addresses1 = get_addresses(coordinates_list, api_key=\"AIzaSyCZtAhDqvbXYdPOxOh_B7yJzRwQaXI0QCc\")\n",
    "    numeros_polygone1.append(i)\n",
    "    adresses1.append(addresses1[0])\n",
    "\n",
    "numeros_polygone1 = []\n",
    "adresses1 = []\n",
    "df[\"numero_polygone\"] = numeros_polygone1\n",
    "df[\"adresse_google\"] = adresses1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Hambourg.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfilter_the_positive_containers_1\u001b[39;00m \u001b[39mimport\u001b[39;00m\u001b[39m*\u001b[39m\n\u001b[1;32m      4\u001b[0m df_terminaux \u001b[39m=\u001b[39m read_csv(\u001b[39m\"\u001b[39m\u001b[39mterminal_list_for_co2.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m\"\u001b[39m\u001b[39mHambourg.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m df[\u001b[39m\"\u001b[39m\u001b[39mgeometry\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m=\u001b[39mshapely\u001b[39m.\u001b[39mwkt\u001b[39m.\u001b[39mloads(df[\u001b[39m\"\u001b[39m\u001b[39mwkt\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m      7\u001b[0m df[\u001b[39m\"\u001b[39m\u001b[39mcenter_point\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m\"\u001b[39m\u001b[39mgeometry\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: x\u001b[39m.\u001b[39mcentroid)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.11/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.11/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[1;32m    579\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_engine(f, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mengine)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m   1662\u001b[0m     f,\n\u001b[1;32m   1663\u001b[0m     mode,\n\u001b[1;32m   1664\u001b[0m     encoding\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m),\n\u001b[1;32m   1665\u001b[0m     compression\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mcompression\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m),\n\u001b[1;32m   1666\u001b[0m     memory_map\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mmemory_map\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m),\n\u001b[1;32m   1667\u001b[0m     is_text\u001b[39m=\u001b[39mis_text,\n\u001b[1;32m   1668\u001b[0m     errors\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mencoding_errors\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mstrict\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m   1669\u001b[0m     storage_options\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mstorage_options\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m),\n\u001b[1;32m   1670\u001b[0m )\n\u001b[1;32m   1671\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.11/site-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\n\u001b[1;32m    860\u001b[0m             handle,\n\u001b[1;32m    861\u001b[0m             ioargs\u001b[39m.\u001b[39mmode,\n\u001b[1;32m    862\u001b[0m             encoding\u001b[39m=\u001b[39mioargs\u001b[39m.\u001b[39mencoding,\n\u001b[1;32m    863\u001b[0m             errors\u001b[39m=\u001b[39merrors,\n\u001b[1;32m    864\u001b[0m             newline\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    865\u001b[0m         )\n\u001b[1;32m    866\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Hambourg.csv'"
     ]
    }
   ],
   "source": [
    "from utils.filter_the_positive_containers import*\n",
    "\n",
    "\n",
    "df_terminaux = read_csv(\"terminal_list_for_co2.csv\")\n",
    "df = pd.read_csv(\"Hambourg.csv\")\n",
    "df[\"geometry\"]=shapely.wkt.loads(df[\"wkt\"])\n",
    "df[\"center_point\"] = df[\"geometry\"].apply(lambda x: x.centroid)\n",
    "df[\"center_lat\"] = df[\"center_point\"].apply(lambda x: x.y)\n",
    "df[\"center_lon\"] = df[\"center_point\"].apply(lambda x: x.x)\n",
    "#df_with_address[\"geometry\"]=shapely.wkt.loads(df_with_address[\"wkt\"])\n",
    "df[\"port\"] = df[\"closest_terminal\"].apply(lambda x: x[:-5])\n",
    "df[\"color\"] = df.apply(zonecolors, axis=1)\n",
    "df[\"name_zone\"] = df.apply(zonename, axis=1)\n",
    "# Créer des listes pour stocker les numéros de polygone et les adresses\n",
    "\n",
    "    \n",
    "\n",
    "df_with_address = df\n",
    "df_with_address[\"on_water\"] =  df_with_address[\"wkt\"].apply(lambda x: filter_polygon_on_sea(x))\n",
    "#suppression des polygones qui sont sur la mer\n",
    "df_wrong = df_with_address.loc[df_with_address[\"on_water\"] == True].reset_index(drop=True).copy()\n",
    "# on garde le reste\n",
    "df_no_sea = df_with_address.loc[df_with_address[\"on_water\"] == False].reset_index(drop=True).copy()\n",
    "df_with_address = df_no_sea\n",
    "df_douane = df_with_address.loc[df_with_address[\"name_zone\"]==\"custom office\"].copy().reset_index(drop=True)\n",
    "df_douane[\"port\"] = df_douane[\"closest_terminal\"].apply(lambda x: x[:-5])\n",
    "\n",
    "df_with_address_not_cust = df_with_address.loc[(df_with_address[\"name_zone\"] != \"custom office\") ].copy().reset_index(drop=True)\n",
    "df_with_address_not_cust[\"distance_to_water\"] =  df_with_address_not_cust[[\"center_lat\",\"center_lon\"]].apply(lambda x: distance_poly_plus_proche(float(x.center_lat),float(x.center_lon)),axis=1)\n",
    "seuil = 0.2  # Le seuil de distance que vous souhaitez utiliser içi(200m)\n",
    "\n",
    "df_kept  =  df_with_address_not_cust.loc[(df_with_address_not_cust[\"distance_to_water\"] <=seuil) | (df_with_address_not_cust[\"name_zone\"] != \"loading quay\")].reset_index(drop=True).copy()\n",
    "df_no_kept =  df_with_address_not_cust.loc[(df_with_address_not_cust[\"distance_to_water\"] >seuil) & (df_with_address_not_cust[\"name_zone\"] == \"loading quay\")].reset_index(drop=True).copy()\n",
    "df_with_address_not_cust_with_terminal_gps = df_kept.merge(df_terminaux[[\"latitude\",\"longitude\",\"name\"]], left_on='port', right_on='name', how='left').copy()\n",
    "#df_with_address_not_cust_with_terminal_gps = df_with_address_not_cust_with_terminal_gps.drop(columns=[\"wkt\",\"polygon\"])\n",
    "df_with_address_not_cust_with_terminal_gps[\"point_terminal\"] = df_with_address_not_cust_with_terminal_gps[[\"latitude\",\"longitude\"]].apply(lambda x: Point(x.longitude,x.latitude),axis=1)\n",
    "df_with_address_not_cust_with_terminal_gps[\"dist_to_point\"] = df_with_address_not_cust_with_terminal_gps[[\"geometry\",\"point_terminal\"]].apply(lambda x : x.geometry.distance(x.point_terminal)*111,axis=1)\n",
    "df_with_address_not_cust_with_terminal_gps.tail(5)\n",
    "df_with_address_not_cust_with_terminal_gps_keep, df_with_address_not_cust_with_terminal_gps_no_keep= filter_dataframe_by_distance(df_with_address_not_cust_with_terminal_gps, 8)\n",
    "df_with_address_not_cust_with_terminal_gps_keep['number_appearance'] = df_with_address_not_cust_with_terminal_gps_keep.groupby('port')['port'].transform('size')\n",
    "# Step 1: Count the occurrences of each address\n",
    "address_counts = df_with_address_not_cust_with_terminal_gps_keep['adresse_google'].value_counts()\n",
    "\n",
    "# Step 2: Split the DataFrame into two groups based on address counts\n",
    "df_1 = df_with_address_not_cust_with_terminal_gps_keep[df_with_address_not_cust_with_terminal_gps_keep['adresse_google'].isin(address_counts[address_counts > 1].index)].copy().reset_index(drop=True)\n",
    "df_2 = df_with_address_not_cust_with_terminal_gps_keep[df_with_address_not_cust_with_terminal_gps_keep['adresse_google'].isin(address_counts[address_counts <= 1].index)].copy().reset_index(drop=True)\n",
    "\n",
    "# Now, df_1 contains rows with addresses that appear at least 6 times,\n",
    "# and df_2 contains rows with addresses that appear less than 6 times.\n",
    "total_adress = df_1['adresse_google'].unique()\n",
    "len(total_adress)\n",
    "total_adress_embedding = [ model.encode(i, convert_to_tensor=True) for i in total_adress]\n",
    "print(len(total_adress_embedding))\n",
    "if len(df.polygon) >500:\n",
    "    print(\"we have a big port\")\n",
    "    df_not_kept1 = pd.DataFrame([])\n",
    "    df_kept1 = pd.DataFrame([])\n",
    "    for ind,row in df_2.iterrows():\n",
    "        adress_embed = model.encode(row[\"adresse_google\"] , convert_to_tensor=True)\n",
    "        list_similarity_percentage = [ util.cos_sim(i, adress_embed).item() for i in total_adress_embedding]\n",
    "        if max(list_similarity_percentage) < 0.85 and row[\"dist_to_point\"] > 1.5:\n",
    "            df_not_kept1 = pd.concat([df_not_kept1,df_2.iloc[[ind]]]).reset_index(drop=True)\n",
    "        else :\n",
    "            df_kept1 = pd.concat([df_kept1,df_2.iloc[[ind]]]).reset_index(drop=True)\n",
    "else:\n",
    "    print(\"we have a Small port\")\n",
    "    df_not_kept1 = pd.DataFrame([])\n",
    "    df_kept1 = pd.DataFrame([])\n",
    "    for ind,row in df_2.iterrows():\n",
    "        adress_embed = model.encode(row[\"adresse_google\"] , convert_to_tensor=True)\n",
    "        list_similarity_percentage = [ util.cos_sim(i, adress_embed).item() for i in total_adress_embedding]\n",
    "        if max(list_similarity_percentage) < 0.9 and row[\"dist_to_point\"] > 1:\n",
    "            df_not_kept1 = pd.concat([df_not_kept1,df_2.iloc[[ind]]]).reset_index(drop=True)\n",
    "        else :\n",
    "            df_kept1 = pd.concat([df_kept1,df_2.iloc[[ind]]]).reset_index(drop=True)\n",
    "df_filter = pd.concat([df_kept1,df_1,df_douane]).reset_index(drop=True)\n",
    "df_not_kept = pd.concat([df_with_address_not_cust_with_terminal_gps_no_keep,df_no_kept,df_not_kept1,df_wrong]).reset_index(drop=True)\n",
    "print(f\"pourcentage gardé:{(len(df_filter)/len(df))*100}\")\n",
    "print(f\"pourcentage supprimé:{(len(df_not_kept)/len(df))*100}\")\n",
    "\n",
    "df_map = df_filter\n",
    "m3 = folium.Map((51.937957, 4.202678), zoom_start=5)\n",
    "def style_function(feature):\n",
    "    name = feature['properties']['name']\n",
    "    color = df_map.loc[df_map['name_zone'] == name, 'color'].iloc[0]\n",
    "    return color\n",
    "for i in df_map.name_zone.unique():\n",
    "    df_i_i = df_map.loc[df_map[\"name_zone\"] == i].reset_index()#drop=True\n",
    "    for ind, poly in enumerate(df_i_i.geometry):\n",
    "        coordinates = list(poly.exterior.coords)\n",
    "        print_polygon = Polygon(coordinates)\n",
    "        polygon = gpd.GeoDataFrame(index=[0], crs='epsg:4326', geometry=[print_polygon])\n",
    "        polygon[\"name\"] = i\n",
    "        #folium.GeoJson(polygon, name=i, style_function=style_function).add_to(m3)\n",
    "        new_poup = folium.Popup(str(df_i_i[\"numero_polygone\"][ind]) + \":\" + str(df_i_i[\"center_point\"][ind]))\n",
    "        geojson =  folium.GeoJson(polygon, name=i,popup=new_poup, style_function=style_function)\n",
    "        new_poup.add_to(geojson)\n",
    "        geojson.add_to(m3)\n",
    "m3.save(\"Hambourg_after_filter.html\")\n",
    "\n",
    "df_map = df_not_kept\n",
    "m3 = folium.Map((51.937957, 4.202678), zoom_start=5)\n",
    "def style_function(feature):\n",
    "    name = feature['properties']['name']\n",
    "    color = df_map.loc[df_map['name_zone'] == name, 'color'].iloc[0]\n",
    "    return color\n",
    "for i in df_map.name_zone.unique():\n",
    "    df_i_i = df_map.loc[df_map[\"name_zone\"] == i].reset_index(drop=True)\n",
    "    for ind, poly in enumerate(df_i_i.geometry):\n",
    "        coordinates = list(poly.exterior.coords)\n",
    "        print_polygon = Polygon(coordinates)\n",
    "        polygon = gpd.GeoDataFrame(index=[0], crs='epsg:4326', geometry=[print_polygon])\n",
    "        polygon[\"name\"] = i\n",
    "        new_poup = folium.Popup(str(df_i_i[\"numero_polygone\"][ind]) + \":\" + str(df_i_i[\"center_point\"][ind]))\n",
    "        geojson =  folium.GeoJson(polygon, name=i,popup=new_poup, style_function=style_function)\n",
    "        new_poup.add_to(geojson)\n",
    "        geojson.add_to(m3)\n",
    "m3.save(\"Hambourg_no_kept.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
