{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psycopg2-binary==2.9.7 in c:\\users\\assane sarr\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from -r requirements.txt (line 1)) (2.9.7)\n",
      "Requirement already satisfied: googlemaps==4.10.0 in c:\\users\\assane sarr\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from -r requirements.txt (line 2)) (4.10.0)\n",
      "Requirement already satisfied: sklearn3==0.0.1 in c:\\users\\assane sarr\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from -r requirements.txt (line 3)) (0.0.1)\n",
      "Requirement already satisfied: boto3==1.28.28 in c:\\users\\assane sarr\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from -r requirements.txt (line 4)) (1.28.28)\n",
      "Requirement already satisfied: requests<3.0,>=2.20.0 in c:\\users\\assane sarr\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from googlemaps==4.10.0->-r requirements.txt (line 2)) (2.31.0)\n",
      "Requirement already satisfied: botocore<1.32.0,>=1.31.28 in c:\\users\\assane sarr\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from boto3==1.28.28->-r requirements.txt (line 4)) (1.31.28)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\assane sarr\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from boto3==1.28.28->-r requirements.txt (line 4)) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in c:\\users\\assane sarr\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from boto3==1.28.28->-r requirements.txt (line 4)) (0.6.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\assane sarr\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from botocore<1.32.0,>=1.31.28->boto3==1.28.28->-r requirements.txt (line 4)) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in c:\\users\\assane sarr\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from botocore<1.32.0,>=1.31.28->boto3==1.28.28->-r requirements.txt (line 4)) (1.26.16)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\assane sarr\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from requests<3.0,>=2.20.0->googlemaps==4.10.0->-r requirements.txt (line 2)) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\assane sarr\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from requests<3.0,>=2.20.0->googlemaps==4.10.0->-r requirements.txt (line 2)) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\assane sarr\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from requests<3.0,>=2.20.0->googlemaps==4.10.0->-r requirements.txt (line 2)) (2023.7.22)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\assane sarr\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.32.0,>=1.31.28->boto3==1.28.28->-r requirements.txt (line 4)) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "from utils.filter_the_positive_containers import*\n",
    "df_terminaux = pd.read_csv(\"C:/Users/ASSANE SARR/Desktop/Stage REEFERPULSE/terminal_list_for_co2.csv\")\n",
    "df = pd.read_csv(\"C:/Users/ASSANE SARR/Downloads/Hambourg.csv\")\n",
    "df[\"geometry\"]=shapely.wkt.loads(df[\"wkt\"])\n",
    "df[\"center_point\"] = df[\"geometry\"].apply(lambda x: x.centroid)\n",
    "df[\"center_lat\"] = df[\"center_point\"].apply(lambda x: x.y)\n",
    "df[\"center_lon\"] = df[\"center_point\"].apply(lambda x: x.x)\n",
    "#df_with_address[\"geometry\"]=shapely.wkt.loads(df_with_address[\"wkt\"])\n",
    "df[\"port\"] = df[\"closest_terminal\"].apply(lambda x: x[:-5])\n",
    "df[\"color\"] = df.apply(zonecolors, axis=1)\n",
    "df[\"name_zone\"] = df.apply(zonename, axis=1)\n",
    "# Créer des listes pour stocker les numéros de polygone et les adresses\n",
    "numeros_polygone1 = []\n",
    "adresses1 = []\n",
    "#latitude = []\n",
    "#longitude = []\n",
    "# Utilisation d'une expression régulière pour extraire les coordonnées numériques\n",
    "#pattern = r'(\\d+\\.\\d+) (\\d+\\.\\d+)'\n",
    "\n",
    "for i in range(0,len(df.center_lat)):\n",
    "    #match = re.search(pattern, polygon_str)\n",
    "    coordinates_list = [(df.center_lat[i], df.center_lon[i])]\n",
    "    addresses1 = get_addresses(coordinates_list, api_key=\"AIzaSyCZtAhDqvbXYdPOxOh_B7yJzRwQaXI0QCc\")\n",
    "    numeros_polygone1.append(i)\n",
    "    adresses1.append(addresses1[0])\n",
    "    \n",
    "\n",
    "#df_port_dk[\"latitude\"] = latitude\n",
    "#df_port_dk[\"longitude\"] = longitude\n",
    "df[\"numero_polygone\"] = numeros_polygone1\n",
    "df[\"adresse_google\"] = adresses1\n",
    "\n",
    "df_with_address = df\n",
    "df_with_address[\"on_water\"] =  df_with_address[\"wkt\"].apply(lambda x: filter_polygon_on_sea(x))\n",
    "#suppression des polygones qui sont sur la mer\n",
    "df_wrong = df_with_address.loc[df_with_address[\"on_water\"] == True].reset_index(drop=True).copy()\n",
    "# on garde le reste\n",
    "df_no_sea = df_with_address.loc[df_with_address[\"on_water\"] == False].reset_index(drop=True).copy()\n",
    "df_with_address = df_no_sea\n",
    "df_douane = df_with_address.loc[df_with_address[\"name_zone\"]==\"custom office\"].copy().reset_index(drop=True)\n",
    "df_douane[\"port\"] = df_douane[\"closest_terminal\"].apply(lambda x: x[:-5])\n",
    "#df_douane.head()\n",
    "#df_with_address_quay = df_with_address.loc[df_with_address[\"name_zone\"]==\"loading quay\"].copy().reset_index(drop=True)\n",
    "#df_with_address_quay[\"port\"] = df_with_address_quay[\"closest_terminal\"].apply(lambda x: x[:-5])\n",
    "#df_with_address_quay.head()\n",
    "\n",
    "df_with_address_not_cust = df_with_address.loc[(df_with_address[\"name_zone\"] != \"custom office\") ].copy().reset_index(drop=True)\n",
    "df_with_address_not_cust[\"distance_to_water\"] =  df_with_address_not_cust[[\"center_lat\",\"center_lon\"]].apply(lambda x: distance_poly_plus_proche(float(x.center_lat),float(x.center_lon)),axis=1)\n",
    "seuil = 0.2  # Le seuil de distance que vous souhaitez utiliser içi(200m)\n",
    "\n",
    "df_kept  =  df_with_address_not_cust.loc[(df_with_address_not_cust[\"distance_to_water\"] <=seuil) | (df_with_address_not_cust[\"name_zone\"] != \"loading quay\")].reset_index(drop=True).copy()\n",
    "df_no_kept =  df_with_address_not_cust.loc[(df_with_address_not_cust[\"distance_to_water\"] >seuil) & (df_with_address_not_cust[\"name_zone\"] == \"loading quay\")].reset_index(drop=True).copy()\n",
    "df_with_address_not_cust_with_terminal_gps = df_kept.merge(df_terminaux[[\"latitude\",\"longitude\",\"name\"]], left_on='port', right_on='name', how='left').copy()\n",
    "#df_with_address_not_cust_with_terminal_gps = df_with_address_not_cust_with_terminal_gps.drop(columns=[\"wkt\",\"polygon\"])\n",
    "df_with_address_not_cust_with_terminal_gps[\"point_terminal\"] = df_with_address_not_cust_with_terminal_gps[[\"latitude\",\"longitude\"]].apply(lambda x: Point(x.longitude,x.latitude),axis=1)\n",
    "df_with_address_not_cust_with_terminal_gps[\"dist_to_point\"] = df_with_address_not_cust_with_terminal_gps[[\"geometry\",\"point_terminal\"]].apply(lambda x : x.geometry.distance(x.point_terminal)*111,axis=1)\n",
    "df_with_address_not_cust_with_terminal_gps.tail(5)\n",
    "df_with_address_not_cust_with_terminal_gps_keep, df_with_address_not_cust_with_terminal_gps_no_keep= filter_dataframe_by_distance(df_with_address_not_cust_with_terminal_gps, 8)\n",
    "df_with_address_not_cust_with_terminal_gps_keep['number_appearance'] = df_with_address_not_cust_with_terminal_gps_keep.groupby('port')['port'].transform('size')\n",
    "# Step 1: Count the occurrences of each address\n",
    "address_counts = df_with_address_not_cust_with_terminal_gps_keep['adresse_google'].value_counts()\n",
    "\n",
    "# Step 2: Split the DataFrame into two groups based on address counts\n",
    "df_1 = df_with_address_not_cust_with_terminal_gps_keep[df_with_address_not_cust_with_terminal_gps_keep['adresse_google'].isin(address_counts[address_counts > 1].index)].copy().reset_index(drop=True)\n",
    "df_2 = df_with_address_not_cust_with_terminal_gps_keep[df_with_address_not_cust_with_terminal_gps_keep['adresse_google'].isin(address_counts[address_counts <= 1].index)].copy().reset_index(drop=True)\n",
    "\n",
    "# Now, df_1 contains rows with addresses that appear at least 6 times,\n",
    "# and df_2 contains rows with addresses that appear less than 6 times.\n",
    "total_adress = df_1['adresse_google'].unique()\n",
    "len(total_adress)\n",
    "total_adress_embedding = [ model.encode(i, convert_to_tensor=True) for i in total_adress]\n",
    "print(len(total_adress_embedding))\n",
    "if len(df.polygon) >500:\n",
    "    print(\"we have a big port\")\n",
    "    df_not_kept1 = pd.DataFrame([])\n",
    "    df_kept1 = pd.DataFrame([])\n",
    "    for ind,row in df_2.iterrows():\n",
    "        adress_embed = model.encode(row[\"adresse_google\"] , convert_to_tensor=True)\n",
    "        list_similarity_percentage = [ util.cos_sim(i, adress_embed).item() for i in total_adress_embedding]\n",
    "        if max(list_similarity_percentage) < 0.85 and row[\"dist_to_point\"] > 1.5:\n",
    "            df_not_kept1 = pd.concat([df_not_kept1,df_2.iloc[[ind]]]).reset_index(drop=True)\n",
    "        else :\n",
    "            df_kept1 = pd.concat([df_kept1,df_2.iloc[[ind]]]).reset_index(drop=True)\n",
    "else:\n",
    "    print(\"we have a Small port\")\n",
    "    df_not_kept1 = pd.DataFrame([])\n",
    "    df_kept1 = pd.DataFrame([])\n",
    "    for ind,row in df_2.iterrows():\n",
    "        adress_embed = model.encode(row[\"adresse_google\"] , convert_to_tensor=True)\n",
    "        list_similarity_percentage = [ util.cos_sim(i, adress_embed).item() for i in total_adress_embedding]\n",
    "        if max(list_similarity_percentage) < 0.9 and row[\"dist_to_point\"] > 1:\n",
    "            df_not_kept1 = pd.concat([df_not_kept1,df_2.iloc[[ind]]]).reset_index(drop=True)\n",
    "        else :\n",
    "            df_kept1 = pd.concat([df_kept1,df_2.iloc[[ind]]]).reset_index(drop=True)\n",
    "df_filter = pd.concat([df_kept1,df_1,df_douane]).reset_index(drop=True)\n",
    "df_not_kept = pd.concat([df_with_address_not_cust_with_terminal_gps_no_keep,df_no_kept,df_not_kept1,df_wrong]).reset_index(drop=True)\n",
    "print(f\"pourcentage gardé:{(len(df_filter)/len(df))*100}\")\n",
    "print(f\"pourcentage supprimé:{(len(df_not_kept)/len(df))*100}\")\n",
    "\n",
    "df_map = df_filter\n",
    "m3 = folium.Map((51.937957, 4.202678), zoom_start=5)\n",
    "def style_function(feature):\n",
    "    name = feature['properties']['name']\n",
    "    color = df_map.loc[df_map['name_zone'] == name, 'color'].iloc[0]\n",
    "    return color\n",
    "for i in df_map.name_zone.unique():\n",
    "    df_i_i = df_map.loc[df_map[\"name_zone\"] == i].reset_index()#drop=True\n",
    "    for ind, poly in enumerate(df_i_i.geometry):\n",
    "        coordinates = list(poly.exterior.coords)\n",
    "        print_polygon = Polygon(coordinates)\n",
    "        polygon = gpd.GeoDataFrame(index=[0], crs='epsg:4326', geometry=[print_polygon])\n",
    "        polygon[\"name\"] = i\n",
    "        #folium.GeoJson(polygon, name=i, style_function=style_function).add_to(m3)\n",
    "        new_poup = folium.Popup(str(df_i_i[\"numero_polygone\"][ind]) + \":\" + str(df_i_i[\"center_point\"][ind]))\n",
    "        geojson =  folium.GeoJson(polygon, name=i,popup=new_poup, style_function=style_function)\n",
    "        new_poup.add_to(geojson)\n",
    "        geojson.add_to(m3)\n",
    "m3.save(\"Hambourg_after_filter.html\")\n",
    "\n",
    "df_map = df_not_kept\n",
    "m3 = folium.Map((51.937957, 4.202678), zoom_start=5)\n",
    "def style_function(feature):\n",
    "    name = feature['properties']['name']\n",
    "    color = df_map.loc[df_map['name_zone'] == name, 'color'].iloc[0]\n",
    "    return color\n",
    "for i in df_map.name_zone.unique():\n",
    "    df_i_i = df_map.loc[df_map[\"name_zone\"] == i].reset_index(drop=True)\n",
    "    for ind, poly in enumerate(df_i_i.geometry):\n",
    "        coordinates = list(poly.exterior.coords)\n",
    "        print_polygon = Polygon(coordinates)\n",
    "        polygon = gpd.GeoDataFrame(index=[0], crs='epsg:4326', geometry=[print_polygon])\n",
    "        polygon[\"name\"] = i\n",
    "        new_poup = folium.Popup(str(df_i_i[\"numero_polygone\"][ind]) + \":\" + str(df_i_i[\"center_point\"][ind]))\n",
    "        geojson =  folium.GeoJson(polygon, name=i,popup=new_poup, style_function=style_function)\n",
    "        new_poup.add_to(geojson)\n",
    "        geojson.add_to(m3)\n",
    "m3.save(\"Hambourg_no_kept.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
